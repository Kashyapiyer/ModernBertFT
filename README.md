**This code is solemly developed using personal colab resources **


Reference Datasets : 
[profanityclean](https://huggingface.co/datasets/tarekziade/profanity-clean)
[surgeaitoxicity](https://github.com/surge-ai/toxicity/blob/main/toxicity_en.csv)

Model considered for finetuning: https://huggingface.co/answerdotai/ModernBERT-base

```
Citation :
@misc{modernbert,
      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, 
      author={Benjamin Warner and Antoine Chaffin and Benjamin Clavié and Orion Weller and Oskar Hallström and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},
      year={2024},
      eprint={2412.13663},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.13663}, 
}
```
